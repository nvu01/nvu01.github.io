<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://nvu01.github.io//feed.xml" rel="self" type="application/atom+xml" /><link href="https://nvu01.github.io//" rel="alternate" type="text/html" hreflang="en" /><updated>2025-11-20T23:52:30+00:00</updated><id>https://nvu01.github.io//feed.xml</id><title type="html">Ngan Vu</title><subtitle>Portfolio of Ngan Vu
</subtitle><author><name>Ngan Vu</name><email>&lt;nganvu2601@gmail.com&gt;</email></author><entry><title type="html">Power BI Report: Analyzing LapCat vs LapDog Performance at Waggle</title><link href="https://nvu01.github.io//featured_projects/2025-06-16-waggle-power-bi-report/" rel="alternate" type="text/html" title="Power BI Report: Analyzing LapCat vs LapDog Performance at Waggle" /><published>2025-06-16T00:00:00+00:00</published><updated>2025-11-20T23:52:05+00:00</updated><id>https://nvu01.github.io//featured_projects/waggle-power-bi-report</id><content type="html" xml:base="https://nvu01.github.io//featured_projects/2025-06-16-waggle-power-bi-report/"><![CDATA[<p>In this project, I analyzed user and pet data from Waggle, a pet-tech startup known for its innovative smart devices for pets. After the success of the LapDog fitness collar for dogs, Waggle launched a field test of its feline counterpart called ‚ÄúLapCat‚Äù. My report aims to help stakeholders understand how the new device is performing and whether it justifies further development.</p>

<p><a href="https://github.com/nvu01/Waggle-PowerBI-Report" target="_blank" rel="noopener">
  <i class="icon-github"></i> GitHub Repo
</a></p>

<ul class="large-only" id="markdown-toc">
  <li><a href="#datasets-overview" id="markdown-toc-datasets-overview">Datasets Overview</a></li>
  <li><a href="#business-requirements" id="markdown-toc-business-requirements">Business Requirements</a></li>
  <li><a href="#the-report" id="markdown-toc-the-report">The Report</a>    <ul>
      <li><a href="#explore-the-interactive-report" id="markdown-toc-explore-the-interactive-report">Explore the Interactive Report</a></li>
      <li><a href="#demo-video" id="markdown-toc-demo-video">Demo Video</a></li>
      <li><a href="#report-layout-overview" id="markdown-toc-report-layout-overview">Report Layout Overview</a></li>
    </ul>
  </li>
  <li><a href="#key-findings" id="markdown-toc-key-findings">Key Findings</a></li>
  <li><a href="#final-thoughts" id="markdown-toc-final-thoughts">Final Thoughts</a></li>
</ul>

<h2 id="datasets-overview">Datasets Overview</h2>

<p>The data used in this project was provided by Udacity as part of their Data Analyst Nanodegree Program. The datasets were already cleaned and curated to simulate real-world business scenarios, allowing me to focus on analysis, visualization, and stakeholder communication.</p>

<p><strong>Data Model</strong></p>

<p><img src="/assets/img/posts/5/data-model.png" alt="description" style="width: 100%; display: block; margin: 1rem 0; text-align: left;" /></p>

<h2 id="business-requirements">Business Requirements</h2>

<p>Waggle stakeholders posed strategic questions to evaluate LapCat‚Äôs market performance and user reception:</p>

<p><strong>From the CEO:</strong></p>
<ul>
  <li>Did average daily steps increase for cats wearing the LapCat, as they did for dogs with the LapDog?</li>
  <li>Were LapCat users as satisfied with the product as LapDog users?</li>
</ul>

<p><strong>From the Product Team:</strong></p>
<ul>
  <li>What demographic insights can we uncover about Waggle families and their pets?</li>
</ul>

<p><strong>Design &amp; Usability Requirements:</strong>
The report also needed to meet Waggle‚Äôs design standards:</p>

<ul>
  <li>A branded header on each page, showing the page title and Waggle logo</li>
  <li>Slicers positioned on the left or beneath the banner to filter visuals</li>
  <li>Navigation buttons placed consistently for ease of use</li>
  <li>Only Waggle-approved palette colors (plus black, white, and gray tones)</li>
  <li>One reset button per page to clear filters</li>
  <li>Buttons for page-to-page navigation</li>
</ul>

<p><img src="/assets/img/posts/5/color_palette.png" alt="description" width="600" style="display: block; margin: 1rem 0; text-align: left;" /></p>

<h2 id="the-report">The Report</h2>

<h3 id="explore-the-interactive-report">Explore the Interactive Report</h3>

<p>üëâ <a href="https://app.fabric.microsoft.com/view?r=eyJrIjoiN2ZiNThiMGYtNWQxOC00NTliLWFkYTgtYjc5ZDdhOWY3NTVjIiwidCI6IjEwZGVlN2UzLWJjMGQtNGNjNy1iMzZhLWEzZDQzMGEzZGI5ZCIsImMiOjZ9" target="_blank" rel="noopener">Open the dashboard in new tab</a>.</p>

<iframe title="Waggle Report" style="width: 100%; height: 65vh;" src="https://app.fabric.microsoft.com/view?r=eyJrIjoiN2ZiNThiMGYtNWQxOC00NTliLWFkYTgtYjc5ZDdhOWY3NTVjIiwidCI6IjEwZGVlN2UzLWJjMGQtNGNjNy1iMzZhLWEzZDQzMGEzZGI5ZCIsImMiOjZ9" frameborder="0" allowfullscreen="true"></iframe>

<h3 id="demo-video">Demo Video</h3>

<div style="width: 100%; margin: 0 auto;">
  <video width="100%" controls="">
    <source src="/assets/other/Waggle.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
</div>

<h3 id="report-layout-overview">Report Layout Overview</h3>
<p>The Power BI report contains four interactive pages, with consistent navigation and styling. Key interface elements include a slicer panel, reset filters button, and page-specific navigation‚Äîall wrapped in Waggle‚Äôs visual identity.</p>

<p><strong>Page 1: LapDog vs LapCat (Home)</strong></p>

<p>This page directly addresses the CEO‚Äôs questions with side-by-side compareisons:</p>

<ul>
  <li>Average daily step count and satisfaction ratings by device</li>
  <li>Step trends over time by species</li>
  <li>Activity minutes and daily steps by month/year</li>
</ul>

<p><strong>Page 2: Pets Summary</strong></p>

<ul>
  <li>Demographics of Waggle pets: species, breed, age, weight.</li>
  <li>Activity breakdown: steps and active minutes by pet age and weight</li>
</ul>

<p><strong>Page 3: Family Summary</strong></p>

<ul>
  <li>Owner location, household size, and income demographics.</li>
  <li>Visuals show patterns in household types and pet ownership.</li>
</ul>

<p><strong>Page 4: Family Details (Drill-Through)</strong></p>

<ul>
  <li>This is a detailed view of individual households and their pets. This page is accessed by right-clicking entries from the table visual on Page 3.</li>
  <li>Supports deeper insights into user-level data.</li>
</ul>

<p><strong><em>Key Features</em></strong></p>

<ul>
  <li>Waggle-branded theme and layout for consistency</li>
  <li>Dynamic visual swapping for comparing trends</li>
  <li>Filter reset and drill-through functionality</li>
  <li>Built-in slicers for stakeholder exploration</li>
  <li>Used the same colors for dogs and cats across all pages to make comparisons easy and intuitive.</li>
</ul>

<h2 id="key-findings">Key Findings</h2>

<p><strong>LapDog vs LapCat Performance</strong></p>

<ul>
  <li>
    <p>Activity Impact: LapDog devices led to a significant increase in average daily steps for dogs, while LapCat devices showed a decrease in feline activity. In 2020, dogs wearing the LapDog averaged <strong>14,667 steps per day</strong>, compared to only <strong>2,765 steps</strong> for cats.</p>
  </li>
  <li>
    <p>User Satisfaction: LapDog received an average rating of <strong>4.69 stars</strong>, with <strong>85.65%</strong> of users giving a perfect <strong>5/5</strong>. In contrast, LapCat scored an average of <strong>1.64 stars</strong>, with <strong>36.02%</strong> of users rating it just <strong>1/5</strong>. This suggests widespread dissatisfaction with the LapCat experience.</p>
  </li>
</ul>

<p><strong>Pets Summary</strong></p>

<ul>
  <li>
    <p>Population &amp; Breed Distribution: The dataset includes <strong>10,161 dogs</strong> across <strong>19 breeds</strong>, and <strong>1,000 cats</strong> across <strong>13 breeds</strong>. The most common dog breed is the <strong>Golden Retriever</strong>; for cats, it‚Äôs the <strong>American Shorthair</strong>.</p>
  </li>
  <li>
    <p>Age &amp; Weight Trends:</p>
    <ul>
      <li>Dogs are most commonly <strong>age 6</strong>, while cat ages are more evenly spread between <strong>5‚Äì12 years</strong>. <strong>Heavier, mid-aged dogs (50‚Äì100 lbs, ages 3‚Äì10)</strong> exhibit the <strong>highest activity minutes</strong>. Interestingly, average daily steps among all dog age groups follow the same upward trend, from <strong>~8,000 to ~15,000 steps over time</strong>.</li>
      <li>Cat <strong>activity minutes</strong> remain relatively <strong>flat across all weight classes</strong>. <strong>Younger cats (under 3 years)</strong> have the least activity time, <strong>under 3,000 minutes</strong>, while older cats average between <strong>3,000 and 10,000 minutes</strong>. <strong>Step count patterns are inconsistent</strong> and show no clear upward trend across cat age groups.</li>
    </ul>
  </li>
</ul>

<p><strong>Family Summary</strong></p>

<ul>
  <li>Key Averages:
    <ul>
      <li><strong>Average household income:</strong> $523,312</li>
      <li><strong>Average household size:</strong> 2.99</li>
      <li><strong>Average annual pet expenses:</strong> $7,883</li>
    </ul>
  </li>
  <li>Location &amp; Purchase Behavior:
    <ul>
      <li>The majority of customers are located in <strong>California</strong>.</li>
      <li><strong>Waggle.com</strong> is the most common purchase channel.</li>
    </ul>
  </li>
  <li>Household Composition: <strong>54.56%</strong> of households own <strong>three pets</strong>, with fairly <strong>even household size distribution</strong>.</li>
</ul>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>This Power BI project for Waggle was a valuable opportunity for me to focus on data modeling, stakeholder communication, and interactive dashboard design.</p>

<p>While branding and layout help engage stakeholders and make the report approachable, my top priority was always data accuracy and delivering practical, business-relevant insights. A visually polished report earns attention, but clear, correct, and useful insights are what drive decisions.</p>]]></content><author><name>Ngan Vu</name><email>&lt;nganvu2601@gmail.com&gt;</email></author><category term="featured_projects" /><category term="Waggle" /><category term="Power BI report" /><category term="data visualization" /><category term="storytelling" /><category term="LapDog" /><category term="LapCat" /><summary type="html"><![CDATA[An interactive Power BI report that visualizes product performance and user data for Waggle.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://nvu01.github.io//assets/img/posts/5/andrew-s-ouo1hbizWwo-unsplash.jpg" /><media:content medium="image" url="https://nvu01.github.io//assets/img/posts/5/andrew-s-ouo1hbizWwo-unsplash.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Undervalued Stock Scanner: From Raw Data to Smart Insights</title><link href="https://nvu01.github.io//featured_projects/2025-06-04-undervalued-stock-scanner/" rel="alternate" type="text/html" title="Undervalued Stock Scanner: From Raw Data to Smart Insights" /><published>2025-06-04T00:00:00+00:00</published><updated>2025-11-20T23:52:05+00:00</updated><id>https://nvu01.github.io//featured_projects/undervalued-stock-scanner</id><content type="html" xml:base="https://nvu01.github.io//featured_projects/2025-06-04-undervalued-stock-scanner/"><![CDATA[<p>Like many aspiring investors, my husband and I were drawn to the challenge of identifying undervalued stocks because they are like hidden gems with solid fundamentals that the market hasn‚Äôt caught up with yet. What started as a weekend project in Excel became a fully automated pipeline combining Power Query, VBA, and Power BI dashboards.</p>

<p><a href="https://github.com/nvu01/Undervalued-Stock-Scanner" target="_blank" rel="noopener">
  <i class="icon-github"></i> GitHub Repo
</a></p>

<p><strong>Disclaimer: This project is for educational purposes only. The analysis, models, and methods used here are not financial advice. Investing carries risk. Always do your own research or consult with a financial advisor before making investment decisions.</strong></p>

<ul class="large-only" id="markdown-toc">
  <li><a href="#project-at-a-glance" id="markdown-toc-project-at-a-glance">Project at a Glance</a></li>
  <li><a href="#datasets" id="markdown-toc-datasets">Datasets</a></li>
  <li><a href="#what-makes-a-stock-undervalued" id="markdown-toc-what-makes-a-stock-undervalued">What Makes a Stock Undervalued?</a>    <ul>
      <li><a href="#key-fundamentals" id="markdown-toc-key-fundamentals">Key Fundamentals</a></li>
      <li><a href="#how-we-evaluate-stocks-using-these-metrics" id="markdown-toc-how-we-evaluate-stocks-using-these-metrics">How We Evaluate Stocks Using These Metrics</a></li>
    </ul>
  </li>
  <li><a href="#dashboard-that-tells-a-story" id="markdown-toc-dashboard-that-tells-a-story">Dashboard that Tells a Story</a>    <ul>
      <li><a href="#key-features--how-to-use-them" id="markdown-toc-key-features--how-to-use-them">Key Features &amp; How to Use Them</a></li>
    </ul>
  </li>
  <li><a href="#automating-the-mess-turning-chaos-into-clean-data" id="markdown-toc-automating-the-mess-turning-chaos-into-clean-data">Automating the Mess: Turning Chaos into Clean Data</a></li>
  <li><a href="#final-thoughts" id="markdown-toc-final-thoughts">Final Thoughts</a></li>
</ul>

<h2 id="project-at-a-glance">Project at a Glance</h2>

<p>The idea started with my husband‚Äôs interest in value investing and my curiosity for building smarter, faster workflows. He defined the investment logic and I built the engine. Our goal? Identifying potentially undervalued stocks by analyzing key financial metrics across various sectors. The project focuses on three market cap categories (Large Cap, Mid Cap, and Small Cap) and applies statistical methods to assess stock fundamentals.</p>

<h2 id="datasets">Datasets</h2>

<p>We used 11 raw CSV files exported from Thinkorswim, each representing a different sector. These contain financial fundamentals segmented by market cap:</p>

<ul>
  <li>Large-Cap: $10B+</li>
  <li>Mid-Cap: $2B‚Äì$10B</li>
  <li>Small-Cap: $250M‚Äì$2B</li>
</ul>

<p>Each sector includes multiple industries. When comparing stocks, it‚Äôs important to consider market capitalization and industry because a ‚Äúgood‚Äù metric in one industry or market cap can be a red flag in another. We structured the pipeline to retain this nuance throughout the analysis.</p>

<h2 id="what-makes-a-stock-undervalued">What Makes a Stock Undervalued?</h2>

<h3 id="key-fundamentals">Key Fundamentals</h3>

<p>Here‚Äôs a quick overview of the financial metrics we used:</p>

<p><strong>Price-to-Free Cash Flow ratio (P/FCF)</strong></p>

<p>Price-to-Free Cash Flow (P/FCF): A high P/FCF ratio indicates that the specific firm is trading at a high price but is not generating enough free cash flows to justify the price. Smaller price ratios are generally preferred, as they may reveal a firm generating ample cash flows that may not yet be reflected in the price.</p>

<p><strong>Price-to-Book ratio (P/B)</strong></p>

<p>Book Value Per Share (BVPS) shows a company‚Äôs net assets per share. If BVPS is higher than the stock price, the stock may be undervalued. The P/B ratio compares market price to book value. Thus, P/B of 1 means it is traded at exactly where it should be. Higher P/B means it might be overvalued.</p>

<p><strong>Return on Equity (ROE)</strong></p>

<p>This ratio is a gauge of a corporation‚Äôs profitability and how efficiently it generates those profits. The higher the ROE, the better a company is at converting its equity financing into profits.</p>

<p><strong>Return on Assets (ROA)</strong></p>

<p>Return on assets measures how effective a company‚Äôs management is in generating profit from the total assets on its balance sheet. A ROA that rises over time indicates that the company is doing well at increasing its profits with each investment dollar it spends.</p>

<p><strong>Asset-to-equity ratio (A/E)</strong></p>

<p>The asset-to-equity ratio measures a company‚Äôs financial leverage by comparing its total assets to its shareholders‚Äô equity. A higher ratio means more debt and higher financial risk. A lower ratio signals a more conservative, equity-heavy structure.</p>

<p><strong>Price-to-Earnings ratio (P/E)</strong></p>

<p>This metric shows how much investors are paying for each dollar of earnings. A high P/E can signal high growth expectations or overvaluation. A low P/E might suggest undervaluation or strong recent performance.</p>

<h3 id="how-we-evaluate-stocks-using-these-metrics">How We Evaluate Stocks Using These Metrics</h3>
<p>Our evaluation is based on a series of carefully selected financial criteria but here‚Äôs the key: we never compare a stock to the wrong peer group. Each stock is only evaluated relative to the industry average within its own market cap category.
To make industry comparisons fair and statistically sound, we calculate the mean and standard deviation for each metric within the same industry and market cap group, <strong>excluding outliers</strong>. Outliers are identified using the interquartile range (IQR) method, and removed before calculating averages and z-scores.</p>

<p><strong>Preliminary criteria:</strong></p>

<p>To be considered ‚Äúundervalued,‚Äù a stock must meet all of these:</p>
<ul>
  <li>P/FCF &gt; 0 and below the industry average</li>
  <li>P/B &gt; 0 and below the industry average</li>
  <li>A/E &gt; 1 and below the industry average</li>
  <li>ROE &gt; 10%</li>
  <li>ROA &gt; 5%</li>
</ul>

<p><strong>Additional criteria:</strong></p>

<p>We further rank stocks based on these bonus signals:</p>
<ul>
  <li>P/B &lt; 70% of industry average</li>
  <li>ROE &gt; industry average</li>
  <li>ROA &gt; industry average</li>
  <li>P/E &lt; industry average</li>
  <li>P/E between 1 and 25</li>
</ul>

<p>For each criterion met, a stock gets 1 point. The highest score a stock can get is 5 points.</p>

<h2 id="dashboard-that-tells-a-story">Dashboard that Tells a Story</h2>

<p>To make the analysis more accessible, I created an interactive Power BI dashboard where users can explore top-scoring undervalued stocks by sector and market cap, alongside industry averages for key financial metrics.</p>

<p>üëâ <a href="https://app.fabric.microsoft.com/view?r=eyJrIjoiZTkwMDQ0OTEtYjMzZS00ZGQzLThiMWYtNzFlNTliZDUxYjAxIiwidCI6IjEwZGVlN2UzLWJjMGQtNGNjNy1iMzZhLWEzZDQzMGEzZGI5ZCIsImMiOjZ9" target="_blank" rel="noopener">Open the dashboard in new tab</a>.</p>

<iframe title="Undervalued Stock Scanner" style="width: 100%; height: 65vh;" src="https://app.fabric.microsoft.com/view?r=eyJrIjoiZTkwMDQ0OTEtYjMzZS00ZGQzLThiMWYtNzFlNTliZDUxYjAxIiwidCI6IjEwZGVlN2UzLWJjMGQtNGNjNy1iMzZhLWEzZDQzMGEzZGI5ZCIsImMiOjZ9" frameborder="0" allowfullscreen="true"></iframe>

<blockquote class="lead">
  <p>The dashboard functions as a scanner, not a recommendation engine or curated report. While the dashboard  highlights stocks that meet certain criteria and includes summary cards showing the top 3 stocks for each individual metric, it avoids naming a single ‚Äúbest overall‚Äù stock. What‚Äôs considered ‚Äúbest‚Äù can vary widely depending on which metrics an investor prioritizes. This design choice reflects an intentional effort to keep the analysis unbiased and flexible. The goal is to present clean, objective data, not advice.</p>
</blockquote>

<h3 id="key-features--how-to-use-them">Key Features &amp; How to Use Them</h3>

<p>This dashboard showcases stocks that meet preliminary undervaluation criteria, then scores and ranks them to help users identify top opportunities.</p>

<p><strong>1. Filter by Sector and Market Cap:</strong> Use the filters (top-right) to select a sector (e.g., Communication Services) and market cap group (Large, Mid, Small).</p>

<p><strong>2. Dynamic Title:</strong> Page headers update automatically based on your selections, so you always know which stock group you‚Äôre viewing.</p>

<p><strong>3. Buttons:</strong> Move between pages, apply filters, or view instructions using built-in buttons.</p>

<p><strong>4. Score-Based Stock Evaluation:</strong> All stocks shown have already passed the baseline filters. Each stock is then evaluated on five additional criteria. It earns 1 point for each one met, with a maximum score of 5. The score for each stock (0-5) is shown as a horizontal stacked bar with one color-coded block per point.</p>

<p><strong>5. Z-Score Matrix:</strong> Shows how far a stock‚Äôs metric deviates from the industry average. This helps users compare each stock to its peers.</p>

<p>Undervalued metrics:</p>
<ul>
  <li>P/FCF, P/B, A/E, P/E ‚Üí negative z-score is better</li>
  <li>ROE, ROA ‚Üí positive z-score is better</li>
</ul>

<p>Z-scores that go in the ‚Äúwrong‚Äù direction (e.g. high P/E) are grayed out.
Z-scores further from 0 (stronger deviations) are shaded in darker green for easy scanning.</p>

<p><strong>6. Score Distribution Pie Chart:</strong> See how many stocks earned each possible score (0‚Äì5), giving a sense of the dataset‚Äôs overall quality.</p>

<p><strong>7. Summary Cards:</strong> Quick-glance cards show a count of stocks per industry and the top 3 stocks for each metric (based on z-score), ideal for spotting individual leaders in specific financial categories.</p>

<p><strong>8. Metric Breakdown Table:</strong> Scrollable table with detailed financial metrics for each stock.</p>

<p><strong>9. Industry Bar Charts (Page 2):</strong> Compare average fundamentals across sectors and industries to spot which industries are overhyped with lower profitability (ROE, ROA) but higher valuations (P/B, P/E, P/FCF).</p>

<p><strong>10. Tooltips for Context:</strong> Hover over any visual to see context like industry name, market cap, price, score, and financials.</p>

<h2 id="automating-the-mess-turning-chaos-into-clean-data">Automating the Mess: Turning Chaos into Clean Data</h2>

<p>Initially, my husband manually cleaned and analyzed the raw stock data in Excel. But with 11 sector-specific datasets and dozens of formulas, the process quickly became time-consuming, repetitive, and prone to error. Especially with frequent data updates, each refresh could take hours.</p>

<p>I completely re-engineered the process to be fast, accurate, and user-friendly by implementing a set of tools:</p>
<ul>
  <li><strong>Power Query</strong> (20+ custom functions): For automatic cleaning and transformation.</li>
  <li><strong>Advanced Excel formulas</strong>: For metric calculation across all datasets.</li>
  <li><strong>RTD function for real-time stock prices</strong>: the RTD (Real-Time Data) function integrates with the Thinkorswim platform to pull real-time stock prices directly into the workbooks.</li>
  <li><strong>Pivot tables</strong>: To calculate industry averages while filtering outliers.</li>
  <li><strong>VBA Macros</strong>: To refresh all data and recalculate results with a single click.</li>
  <li><strong>Conditional formatting</strong>: To surface strong candidates visually, making stock analysis easier for users.</li>
  <li><strong>Dynamic file paths</strong>: So the project works across any machine without broken links.</li>
</ul>

<iframe title="Undervalued Stock Scanner" style="width: 100%; height: 65vh;" src="https://1drv.ms/v/c/e518921fe1a64d0f/IQQelq_cXxVqRJOBlh08dsrjASQvfprWOkeOTJz0MzTxsoc?width=1920&amp;height=1070" frameborder="0" allowfullscreen="true"></iframe>

<blockquote class="lead">
  <p>The resulting Excel workbooks form the backbone of this project, acting as the central platform that integrates Power Query, advanced formulas, and VBA macros. These files can function independently as stock scanners, even without the dashboard. However, the dashboard plays a crucial role in bringing everything together, presenting the data in a way that‚Äôs more accessible and engaging for non-technical users.</p>
</blockquote>

<p>After processing the data in Excel, I brought the results into Power BI, where I created interactive dashboards that:</p>
<ul>
  <li>Highlight top-ranking undervalued stocks</li>
  <li>Compare metrics across sectors and industries</li>
  <li>Visualize trends in P/E, ROE, ROA, and other key indicators</li>
  <li>Enable non-technical users to explore insights without needing to touch the raw data</li>
</ul>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>This project started with a shared curiosity and turned into a complete, repeatable workflow that saves hours and delivers clear insights. It was as much about collaboration as it was about technology. Working closely with my husband, a non-technical stakeholder, I used an informal agile approach by delivering in small chunks, gathering feedback, and adapting quickly. Clear communication and step-by-step guides I created helped my husband transition smoothly to the new process. I‚Äôm proud to have taken initiative, self-taught new skills ahead of coursework, and built a fully automated system that saves time and drives smarter investment decisions.</p>

<p>The real win? Building something that works for both analysts and non-technical users alike. It reminded me that the best data projects don‚Äôt just analyze ‚Äî they simplify. They turn messy data into clear insights and help people make better, faster decisions. That‚Äôs what I aim to build, every time.</p>]]></content><author><name>Ngan Vu</name><email>&lt;nganvu2601@gmail.com&gt;</email></author><category term="featured_projects" /><category term="dashboard" /><category term="stock market" /><category term="financial market" /><summary type="html"><![CDATA[A collaborative project that combines financial logic with automated Excel and Power BI tools to transform raw data into user-friendly and insight-driven dashboards.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://nvu01.github.io//assets/img/posts/4/adam-smigielski-K5mPtONmpHM-unsplash.jpg" /><media:content medium="image" url="https://nvu01.github.io//assets/img/posts/4/adam-smigielski-K5mPtONmpHM-unsplash.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Socioeconomic Factors Impacting Poverty in U.S. Counties: A Regression Approach</title><link href="https://nvu01.github.io//featured_projects/2025-06-01-poverty-regression-analysis/" rel="alternate" type="text/html" title="Socioeconomic Factors Impacting Poverty in U.S. Counties: A Regression Approach" /><published>2025-06-01T00:00:00+00:00</published><updated>2025-11-20T23:52:05+00:00</updated><id>https://nvu01.github.io//featured_projects/poverty-regression-analysis</id><content type="html" xml:base="https://nvu01.github.io//featured_projects/2025-06-01-poverty-regression-analysis/"><![CDATA[<p>This post summarizes my capstone project for my program in Data Analytics at WGU. I used <strong>multiple linear regression</strong> to investigate how socioeconomic factors influence poverty rates across U.S. counties.</p>

<p><a href="https://github.com/nvu01/BSDA-Capstone-Project" target="_blank" rel="noopener">
  <i class="icon-github"></i> GitHub Repo
</a></p>

<ul class="large-only" id="markdown-toc">
  <li><a href="#project-overview" id="markdown-toc-project-overview">Project Overview</a></li>
  <li><a href="#description-of-dataset" id="markdown-toc-description-of-dataset">Description of Dataset</a></li>
  <li><a href="#data-collection" id="markdown-toc-data-collection">Data Collection</a></li>
  <li><a href="#data-wrangling" id="markdown-toc-data-wrangling">Data Wrangling</a></li>
  <li><a href="#analytical-methods" id="markdown-toc-analytical-methods">Analytical Methods</a></li>
  <li><a href="#project-outcomes" id="markdown-toc-project-outcomes">Project Outcomes</a>    <ul>
      <li><a href="#models-predictive-power-and-significance" id="markdown-toc-models-predictive-power-and-significance">Model‚Äôs Predictive Power and Significance</a></li>
      <li><a href="#models-validity" id="markdown-toc-models-validity">Model‚Äôs Validity</a></li>
      <li><a href="#most-impactful-predictors" id="markdown-toc-most-impactful-predictors">Most Impactful Predictors</a>        <ul>
          <li><a href="#statistical-significance-of-individual-predictors" id="markdown-toc-statistical-significance-of-individual-predictors">Statistical Significance of Individual Predictors</a></li>
          <li><a href="#relative-importance-of-individual-predictors" id="markdown-toc-relative-importance-of-individual-predictors">Relative Importance of Individual Predictors</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#recommended-courses-of-action" id="markdown-toc-recommended-courses-of-action">Recommended Courses of Action</a></li>
  <li><a href="#final-thoughts" id="markdown-toc-final-thoughts">Final Thoughts</a></li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>

<h2 id="project-overview">Project Overview</h2>

<p>Poverty is a multifaceted issue that is influenced by many factors such as income levels, education, employment rates, healthcare access, public programs and housing conditions. These variables interact in ways that are not immediately apparent, and their combined effects on poverty rates can vary widely across different regions. Data analysis provides objective insights rather than relying on surface-level observations. As a data analyst, I wanted to answer a critical question: <strong>What are the most important socioeconomic factors that explain poverty rates in U.S. counties, and how reliably can we model those rates using multiple linear regression?</strong></p>

<p>In this project, I applied a rigorous, statistics approach to analyze data from the U.S. Census Bureau‚Äôs 2023 American Community Survey. The goal was not just to model poverty, but to surface meaningful insights that could guide policy and resource allocation.</p>

<p>I used Jupyter Notebook as the primary development platform. The whole analytical process was done using Python libraries such as pandas for data manipulation, Matplotlib and Seaborn for visualization, NumPy for numerical operations, statsmodels for regression and statistical testing, and scikit-learn for scaling and model validation.</p>

<h2 id="description-of-dataset">Description of Dataset</h2>

<p>The dataset for this project comes from the U.S. Census Bureau‚Äôs 2023 American Community Survey (ACS) 1-Year Estimates, a trusted national source of socioeconomic data. Publicly available on <a href="https://data.census.gov/" target="_blank" rel="noopener">data.census.gov</a>, the datasets include relevant indicators like income, education, unemployment, and public assistance, making it well-suited for analyzing poverty predictors at the county level.</p>

<p>A key limitation is that the poverty rate is based on the Official Poverty Measure (OPM), which doesn‚Äôt factor in regional living costs or non-cash benefits like SNAP or housing subsidies. This could limit how socioeconomic factors are interpreted. According to the Census Bureau‚Äôs 2023 <a href="https://www.census.gov/library/stories/2024/11/supplemental-poverty-measure-visualization.html" target="_blank" rel="noopener">report</a>, the OPM showed a national poverty rate of 11.5%, while the more comprehensive Supplemental Poverty Measure (SPM) reported 12.7%, underscoring the impact of additional costs and public assistance income on poverty assessments.</p>

<p>üëâ <a href="https://www.census.gov/data/developers/data-sets/acs-1year.html" target="_blank" rel="noopener">More information about the U.S. Census Bureau‚Äôs ACS data</a>.</p>

<h2 id="data-collection">Data Collection</h2>

<p>The data were collected using API requests to the U.S. Census Bureau‚Äôs 2023 ACS 1-Year datasets. The data collection process required learning how ACS data is structured. The whole process felt a bit overwhelming at first because the data is structured in ways I wasn‚Äôt used to. So, I spent time going through the Census Bureau‚Äôs documentation to get a clear picture of how everything fit together, including the nuances of table formats, variable naming and the geographic codes like the ucgid.</p>

<p>Initially, I planned to manually search for variable and geographic codes, but I later developed a more efficient method. I created a Python script that searches downloaded metadata files for variable codes based on table IDs and variable labels. This tool handles case and spacing issues, reducing human error. Part of the code was adapted from this 
<a href="https://stackoverflow.com/questions/70383942/filtering-a-column-specify-case-sensitivity" target="_blank" rel="noopener">Stack Overflow thread</a>.</p>

<p>üëâ <a href="/assets/html/retrieving_variable_codes.html" target="_blank" rel="noopener">See my notebook for variable code retrieval</a>.</p>

<h2 id="data-wrangling">Data Wrangling</h2>

<p>Data from ACS tables were saved as .json files, parsed using Python, and combined into a single county-level dataset with Pandas. During data preparation, I renamed coded headers to descriptive variable names, converted data types, and addressed missing or special values by imputing based on 2022 ACS data. I also removed redundant columns and combined related variables to enhance usability. The cleaned dataset was then saved as a .csv file for modeling.</p>

<p>üëâ <a href="/assets/html/data_extraction_&amp;_data_wrangling.html" target="_blank" rel="noopener">See my notebook for data wrangling step</a>.</p>

<h2 id="analytical-methods">Analytical Methods</h2>

<p>I began exploratory data analysis (EDA) with <strong>descriptive statistics</strong> to examine central tendencies and spread. This is followed by calculating a <strong>correlation matrix</strong> and <strong>VIF</strong> to detect multicolinearity. Predictors with a VIF above 5 were either dropped or transformed. To explore relationships between the response and explanatory variables, I used <strong>pairplots</strong> and <strong>scatter plots</strong>. To address nonlinearity and skewness in the data, I applied appropriate transformations based on the distribution and each variable‚Äôs relationship with the response:</p>
<ul>
  <li>Log transformation for heavy right-tails</li>
  <li>Log(x+1) transformation when zeros appeared</li>
  <li>Square root transformation for moderate skew.</li>
</ul>

<p>The main analytical approach was a <strong>multiple linear regression</strong> method, which allowed me to quantify relationships between socioeconomic factors and poverty rates while considering the influence of multiple variables simultaneously. Specifically, the project used <strong>ordinary least squares (OLS)</strong> model which allowed a straightforward interpretation of coefficients, significance testing and model evaluation.</p>

<p>To evaluate the model‚Äôs predictive performance, I used metrics such as <strong>R-squared</strong> on both the training and test sets, <strong>adjusted R-squared</strong>, and <strong>mean squared error (MSE)</strong> on the test set. These metrics provided insight into how well the model fit the training data and how accurately it predicted poverty rates on new, unseen data.</p>

<p>To evaluate the role of socioeconomic factors in explaining poverty rates across U.S. counties, both a global and individual hypothesis testing framework will be used.</p>
<ul>
  <li><strong>Global hypothesis</strong> (F-test): This test evaluates whether the full set of predictors contributes to explaining poverty rates.
    <ul>
      <li>
        <p>Null hypothesis: All regression coefficients are equal to zero.<br />
  <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mn>0</mn></msub><mo>:</mo><mtext>‚ÄÖ‚Ää</mtext><msub><mi>Œ≤</mi><mn>1</mn></msub><mo>=</mo><msub><mi>Œ≤</mi><mn>2</mn></msub><mo>=</mo><mo>‚ãØ</mo><mo>=</mo><msub><mi>Œ≤</mi><mi>k</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">H_0:\;\beta_1=\beta_2=\dots=\beta_k=0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0813em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">Œ≤</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">Œ≤</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="minner">‚ãØ</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">Œ≤</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span></p>
      </li>
      <li>
        <p>Alternative hypothesis: At least one regression coefficient is not equal to zero.<br />
  <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mn>1</mn></msub><mo>:</mo><mtext>‚ÄÖ‚Ää</mtext><mtext>at¬†least¬†one¬†</mtext><msub><mi>Œ≤</mi><mi>i</mi></msub><mo mathvariant="normal">‚â†</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">H_1:\;\text{at least one } \beta_i \ne 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0813em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">at¬†least¬†one¬†</span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">Œ≤</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel"><span class="mrel"><span class="mord vbox"><span class="thinbox"><span class="rlap"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="inner"><span class="mord"><span class="mrel">ÓÄ†</span></span></span><span class="fix"></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span></p>
      </li>
    </ul>
  </li>
  <li><strong>Individual hypotheses</strong> (t-test for each coefficient): These tests assess the significance of each predictor.
    <ul>
      <li>
        <p>Null Hypothesis: The coefficient for predictor i is zero.<br />
  <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mn>0</mn></msub><mo>:</mo><mtext>‚ÄÖ‚Ää</mtext><msub><mi>Œ≤</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">H_0:\;\beta_i = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0813em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">Œ≤</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span></p>
      </li>
      <li>
        <p>Alternative Hypothesis: The coefficient for predictor i is not zero.<br />
  <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mn>0</mn></msub><mo>:</mo><mtext>‚ÄÖ‚Ää</mtext><msub><mi>Œ≤</mi><mi>i</mi></msub><mo mathvariant="normal">‚â†</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">H_0:\;\beta_i \neq 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0813em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">Œ≤</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel"><span class="mrel"><span class="mord vbox"><span class="thinbox"><span class="rlap"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="inner"><span class="mord"><span class="mrel">ÓÄ†</span></span></span><span class="fix"></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span></p>
      </li>
    </ul>
  </li>
</ul>

<p>To evaluate the relative importance of each socioeconomic factor in explaining poverty rates, I also used the standardized coefficients from the multiple linear regression model. These coefficients were obtained by scaling all independent variables using <strong>MinMaxScaler</strong> before fitting the model. This will ensure that the predictors are on the same scale.</p>

<p>For the baseline model and the improved model, I implemented residual analysis to <strong>verify model assumptions</strong> and <strong>ensure the model‚Äôs validity</strong>. OLS regression relies on several assumptions: <strong>linearity</strong> of relationships between predictors and the dependent variable, <strong>homoscedasticit</strong>y (constant variance of residuals), <strong>independence of residual</strong>s, and <strong>normally distributed errors</strong>. The analysis included visualizations such as <strong>residual plots</strong>, <strong>histogram and boxplot of residual</strong>s, and <strong>Q-Q plot</strong>. This diagnostic step helped confirm whether the use of OLS and the resulting hypothesis tests were valid.</p>

<p>Finally, to address issues identified in model diagnostics, I applied several <strong>refinement methods</strong>:</p>
<ul>
  <li><strong>Square root transformation</strong> was applied to the <strong>response variable</strong> to reduce heteroscedasticity and improve linearity. The baseline model uses the raw poverty rate as the dependent variable. That may distort relationships, especially when poverty is skewed.</li>
  <li>An <strong>interaction term</strong> was added to improve model accuracy when the effect of one variable depends on another. A potential interaction term can be identified by asking this question: Do any predictors influence each other‚Äôs effects on the dependent variable? In the baseline model, I‚Äôm assuming that household income has the same effect everywhere, regardless of house values. But what if counties with medium to high income have housing affordability issues? The impact of household income on poverty may differ based on local housing cost.</li>
</ul>

<p><img src="/assets/img/posts/3/interaction_term.png" alt="description" width="550" style="display: block; margin: 0 auto;" /></p>

<p>The drop in poverty rate with increasing income appears steeper for counties with lower house value (blue cluster) than ones with higher house value (orange cluster). This means that the effect of income on poverty varies depending on house values.</p>

<p>Here‚Äôs the interaction term:<br />
<code class="language-plaintext highlighter-rouge">df2['income_x_house_value'] = (df2['log_median_income'] * df2['sqrt_median_house_value'])</code></p>

<ul>
  <li>Finally, <strong>robust standard errors (HC3)</strong> were used to produce more reliable p-values in the presence of heteroscedasticity.</li>
</ul>

<p>Once I transformed the response variable and introduced interaction, I get a clearer, more interpretable model.</p>

<p>üëâ <a href="/assets/html/main_analysis.html" target="_blank" rel="noopener">See my notebook for application of analytical methods</a>.</p>

<h2 id="project-outcomes">Project Outcomes</h2>

<h3 id="models-predictive-power-and-significance">Model‚Äôs Predictive Power and Significance</h3>

<blockquote class="lead">
  <p>The refined model was statistically significant and explained over 83% of the variation in poverty on unseen data.</p>
</blockquote>

<p>My refined regression model worked well and explained 79.6% of the variance in poverty rates in training data and 83.1% in the test set. That‚Äôs high, especially for social data, where perfect predictions are rare.</p>

<p>Since the p-value is well below 0.05, we reject the null hypothesis. This indicates the model as a whole is statistically significant.</p>

<p>Final model summary:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:      sqrt_poverty_rate   R-squared:                       0.796
Model:                            OLS   Adj. R-squared:                  0.793
Method:                 Least Squares   F-statistic:                     312.2
Date:                Wed, 04 Jun 2025   Prob (F-statistic):          2.10e-219
Time:                        00:29:01   Log-Likelihood:                -139.94
No. Observations:                 674   AIC:                             297.9
Df Residuals:                     665   BIC:                             338.5
Df Model:                           8                                         
Covariance Type:                  HC3                                         

Notes:
[1] Standard Errors are heteroscedasticity robust (HC3)
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>R-squared (test set): 0.8310581036269753
Mean Squared Error: 0.07192225315999502
</code></pre></div></div>

<h3 id="models-validity">Model‚Äôs Validity</h3>

<blockquote class="lead">
  <p>Overall, the final model met the linear regression assumptions and its results (coefficients, p-values) can be trusted for inference.</p>
</blockquote>

<p><strong>Residual vs fitted plot:</strong> Was used to evaluate model fit and assumption validity, such as linearity and homoscedasticity.</p>

<p><img src="/assets/img/posts/3/final_residuals_fitted.png" alt="description" width="500" style="display: block; margin: 0 auto;" /></p>

<p>Residuals are randomly scattered around zero with consistent spread across fitted values. This means there is no major heteroscedasticity. A few mild outliers are present.</p>

<p><strong>Residuals vs predictors plots:</strong> Were used to detect potential non-linear relationships or predictors that could benefit from transformations.</p>

<p><img src="/assets/img/posts/3/final_residuals_predictors.png" alt="description" style="width: 100%; display: block; margin: 0 auto;" /></p>

<p>No strong patterns or funnel shapes. This supports the assumption of constant variance and linearity across predictors.</p>

<p><strong>Histogram and boxplot:</strong> Were used to assess whether residuals are normally distributed.</p>

<p><img src="/assets/img/posts/3/final_residuals_normality.png" alt="description" style="width: 100%; display: block; margin: 0 auto;" /></p>

<ul>
  <li>Histogram and boxplot of residuals: Distribution is roughly normal, bell-shaped, and symmetrical. The median is near zero, with a few mild upper-end outliers.</li>
</ul>

<p><strong>Q-Q plot:</strong> Was used to assess normality of residuals by comparing their distribution to a theoretical normal distribution.</p>

<p><img src="/assets/img/posts/3/final_qq.png" alt="description" width="500" style="display: block; margin: 0 auto;" /></p>

<ul>
  <li>Q-Q plot: Residuals closely follow the normal line with a slight deviation in the upper tail, which is acceptable.</li>
</ul>

<h3 id="most-impactful-predictors">Most Impactful Predictors</h3>

<h4 id="statistical-significance-of-individual-predictors">Statistical Significance of Individual Predictors</h4>

<blockquote class="lead">
  <p>Public assistance may still matter, despite not being statistically significant, due to how poverty is defined.</p>
</blockquote>

<p>Predictors‚Äô metrics: <a id="predictors-metrics"></a></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>===========================================================================================
                              coef    std err          z      P&gt;|z|      [0.025      0.975]
-------------------------------------------------------------------------------------------
const                       5.0828      0.125     40.570      0.000       4.837       5.328
health_insurance           -0.3674      0.107     -3.445      0.001      -0.576      -0.158
unemployment_rate           0.8111      0.109      7.411      0.000       0.597       1.026
bachelor_holders            0.1332      0.116      1.149      0.251      -0.094       0.360
public_assistance           0.0735      0.120      0.613      0.540      -0.162       0.309
log_median_income          -5.2078      0.252    -20.651      0.000      -5.702      -4.713
log_public_transit          0.7545      0.095      7.940      0.000       0.568       0.941
sqrt_median_house_value   -18.3796      3.244     -5.666      0.000     -24.738     -12.021
income_x_house_value       20.4523      3.399      6.018      0.000      13.791      27.113
</code></pre></div></div>

<p>Based on the predictors‚Äô p-values in the model‚Äôs summary above:</p>
<ul>
  <li><strong>Significant</strong> predictors included <strong>median income</strong>, <strong>median house value</strong>, <strong>health insurance coverage</strong>, <strong>public transit use</strong>, and an <strong>income √ó house value</strong> interaction.</li>
  <li>Surprisingly, <strong>public assistance rates</strong> and the <strong>percentage of bachelor‚Äôs degree holders</strong> were <strong>not statistically significant</strong> in the final model. This means that these factors have limited unique contribution to predictive power of the model once other variables were controlled.</li>
</ul>

<p>It is important to note that unlike SPM, the OPM does not include non-cash public assistance (like SNAP, housing subsidies, TANF) in its calculation. So counties with high assistance rates may not show reduced poverty under OPM even though aid might be helping in reality.</p>

<h4 id="relative-importance-of-individual-predictors">Relative Importance of Individual Predictors</h4>

<blockquote class="lead">
  <p>Counties with higher home values and incomes tend to have lower poverty rates, but each factor‚Äôs impact weakens as the other increases.</p>
</blockquote>

<p><img src="/assets/img/posts/3/tornado_diagram.png" alt="description" width="650" style="display: block; margin: 0 auto;" /></p>

<ul>
  <li>The interaction between income and house value had the largest impact. A positive coefficient for the interaction term suggests that <strong>as income increases, the negative effect of house value on poverty becomes less strong</strong>, or <strong>as house value increases, the negative effect of income on poverty rate weakens</strong>.</li>
  <li>The most impactful predictors of poverty rates are <strong>median house value</strong> and <strong>median household income</strong> as they both show strong negative relationships. <strong>Counties with higher home values and incomes tend to have lower poverty rates.</strong></li>
  <li>Other variables had smaller coefficients but still measurable effects.
    <ul>
      <li>Higher unemployment associates with higher poverty</li>
      <li>Greater health insurance coverage comes with lower poverty</li>
      <li>More public transit use relates to higher poverty (possibly reflecting urban conditions)</li>
    </ul>
  </li>
  <li>Another interesting finding is that <strong>median household income</strong> in the <strong>base model</strong> had a <strong>positive coefficient</strong> (4.7) (see the <a href="/assets/other/base_OLS Regression Results.txt" target="_blank" rel="noopener">base model summary</a>), which was unexpected because a 
<a href="/assets/img/posts/3/scatter_poverty_house-value.png" target="_blank" rel="noopener">regression plot between poverty rate and median household income</a> suggests otherwise. However, in the <strong>refined model</strong>, the sign of this coefficient flipped due to the interaction term (see <a href="#predictors-metrics">predictors‚Äô metrics</a>). The <strong>negative coefficient</strong> means that higher housing values are associated with lower poverty rates. This adjusted effect of median house values in the final model aligns more with general economic assumptions.</li>
</ul>

<h2 id="recommended-courses-of-action">Recommended Courses of Action</h2>

<p>A local government or policymaker could use this model to identify counties most at risk based on these factors and effectively target resources or policies.</p>
<ul>
  <li>Invest in income growth and housing affordability: These were the strongest predictors of lower poverty. Policies targeting wage growth, job access, and affordable housing could significantly reduce poverty rates.</li>
  <li>Expand access to health insurance: Health insurance coverage was significantly linked to lower poverty. Improving coverage could reduce financial hardship and support poverty reduction efforts.</li>
</ul>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>This project demonstrates how careful data wrangling, transparent statistical methods, and principled diagnostics can produce models that are both insightful and trustworthy. It also highlights the importance of questioning assumptions, especially when working with policy-related data.</p>

<p>Not every result was expected. Public assistance didn‚Äôt come through as a statistically significant predictor likely due to limitations of the Official Poverty Measure (OPM), which does not account for non-cash assistance like SNAP or housing vouchers. That‚Äôs a flaw in the poverty definition itself, not necessarily in the data.</p>

<p>This is a reminder: statistics model relative relationships, not realities. The real world is messier. Numbers can‚Äôt fully capture dignity, barriers, or the trade-offs that people in poverty navigate every day. Still, we can quantify strong patterns, and we should, especially when it helps guide policymakers, public agencies, and community organizations in prioritizing resources and interventions.</p>

<p>If you‚Äôre a policymaker, nonprofit leader, or fellow data scientist: I hope this analysis adds value to your work or sparks new questions worth pursuing.</p>

<h2 id="references">References</h2>
<ul>
  <li>Creamer, J., &amp; King, M. D. (2024, November 14). <a href="https://www.census.gov/library/stories/2024/11/supplemental-poverty-measure-visualization.html">How Do Policies and Expenses Affect Supplemental Poverty Rates?</a></li>
</ul>]]></content><author><name>Ngan Vu</name><email>&lt;nganvu2601@gmail.com&gt;</email></author><category term="featured_projects" /><category term="regression" /><category term="socioeconomic data" /><category term="poverty" /><category term="census data" /><category term="public policy" /><summary type="html"><![CDATA[A regression-based analysis of how socioeconomic variables drive poverty rates across U.S. counties, using data from the U.S. Census Bureau.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://nvu01.github.io//assets/img/posts/3/kostiantyn-li-1sCXwVoqKAw-unsplash.jpg" /><media:content medium="image" url="https://nvu01.github.io//assets/img/posts/3/kostiantyn-li-1sCXwVoqKAw-unsplash.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Preparing Data for Reporting Using PostgreSQL</title><link href="https://nvu01.github.io//featured_projects/2025-05-30-SQL-preparing-data-for-reporting/" rel="alternate" type="text/html" title="Preparing Data for Reporting Using PostgreSQL" /><published>2025-05-30T00:00:00+00:00</published><updated>2025-11-20T23:52:05+00:00</updated><id>https://nvu01.github.io//featured_projects/SQL-preparing-data-for-reporting</id><content type="html" xml:base="https://nvu01.github.io//featured_projects/2025-05-30-SQL-preparing-data-for-reporting/"><![CDATA[<p>This project demonstrates how SQL and PostgreSQL can be used to transform raw transactional data into structured, analysis-ready tables. Using the DVD Rental Dataset, I built a structured workflow to extract, transform, and organize rental data for future insights and analysis.</p>

<p><a href="https://github.com/nvu01/Advanced-Data-Management" target="_blank" rel="noopener">
  <i class="icon-github"></i> GitHub Repo
</a></p>

<ul class="large-only" id="markdown-toc">
  <li><a href="#project-overview" id="markdown-toc-project-overview">Project Overview</a>    <ul>
      <li><a href="#database" id="markdown-toc-database">Database</a></li>
      <li><a href="#business-use-cases" id="markdown-toc-business-use-cases">Business Use Cases</a></li>
      <li><a href="#deliverables" id="markdown-toc-deliverables">Deliverables</a></li>
      <li><a href="#tools--skills-used" id="markdown-toc-tools--skills-used">Tools &amp; Skills Used</a></li>
    </ul>
  </li>
  <li><a href="#key-sql-components" id="markdown-toc-key-sql-components">Key SQL Components</a>    <ul>
      <li><a href="#extracting-year-and-month-with-functions" id="markdown-toc-extracting-year-and-month-with-functions">Extracting Year and Month with Functions</a></li>
      <li><a href="#creating-tables" id="markdown-toc-creating-tables">Creating Tables</a></li>
      <li><a href="#trigger-for-summary-table-updates" id="markdown-toc-trigger-for-summary-table-updates">Trigger for Summary Table Updates</a></li>
      <li><a href="#populating-tables" id="markdown-toc-populating-tables">Populating Tables</a></li>
      <li><a href="#stored-procedure-to-refresh-all-data" id="markdown-toc-stored-procedure-to-refresh-all-data">Stored Procedure to Refresh All Data</a></li>
    </ul>
  </li>
  <li><a href="#automation-with-pgagent" id="markdown-toc-automation-with-pgagent">Automation with pgAgent</a></li>
  <li><a href="#final-thoughts" id="markdown-toc-final-thoughts">Final Thoughts</a></li>
</ul>

<h2 id="project-overview">Project Overview</h2>

<h3 id="database">Database</h3>

<p><strong>Sample Database:</strong> dvdrental.zip containing the database is available for download in my <a href="https://github.com/nvu01/Advanced-Data-Management">GitHub repo</a>
or from <a href="https://neon.tech/postgresql/postgresql-getting-started/postgresql-sample-database">neon.tech</a></p>

<p><strong>Data Model (ERD):</strong> <a href="/assets/img/posts/2/dvdrental_data_model.png" target="_blank">View data model</a></p>

<p><strong>Source Tables Used:</strong> I used two key tables from the DVD Rental sample database:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">rental</code>: Contains information about each film rental.</li>
</ul>

<table>
  <thead>
    <tr>
      <th>Column Name</th>
      <th>Data Type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>rental_id</td>
      <td>integer</td>
    </tr>
    <tr>
      <td>rental_date</td>
      <td>timestamp without time zone</td>
    </tr>
    <tr>
      <td>inventory_id</td>
      <td>integer</td>
    </tr>
    <tr>
      <td>customer_id</td>
      <td>smallint</td>
    </tr>
    <tr>
      <td>return_date</td>
      <td>timestamp without time zone</td>
    </tr>
    <tr>
      <td>staff_id</td>
      <td>smallint</td>
    </tr>
    <tr>
      <td>last_update</td>
      <td>timestamp without time zone</td>
    </tr>
  </tbody>
</table>

<ul>
  <li><code class="language-plaintext highlighter-rouge">category</code>: Provides the category_name for each film.</li>
</ul>

<table>
  <thead>
    <tr>
      <th>Column Name</th>
      <th>Data Type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>category_id</td>
      <td>integer</td>
    </tr>
    <tr>
      <td>name</td>
      <td>character varying</td>
    </tr>
    <tr>
      <td>last_update</td>
      <td>timestamp without time zone</td>
    </tr>
  </tbody>
</table>

<p><strong>Tables Created:</strong></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">detailed_table</code>: A transactional table showing individual rental records with added time and category context.</li>
</ul>

<table>
  <thead>
    <tr>
      <th>Column Name</th>
      <th>Data Type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>rental_id</td>
      <td>integer</td>
    </tr>
    <tr>
      <td>rental_year</td>
      <td>integer</td>
    </tr>
    <tr>
      <td>rental_month</td>
      <td>integer</td>
    </tr>
    <tr>
      <td>category_name</td>
      <td>character varying</td>
    </tr>
  </tbody>
</table>

<ul>
  <li><code class="language-plaintext highlighter-rouge">summary_table</code>: An aggregated table used for reporting the total number of rentals by genre per month.</li>
</ul>

<table>
  <thead>
    <tr>
      <th>Column Name</th>
      <th>Data Type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>rental_year</td>
      <td>integer</td>
    </tr>
    <tr>
      <td>rental_month</td>
      <td>integer</td>
    </tr>
    <tr>
      <td>category_name</td>
      <td>character varying</td>
    </tr>
    <tr>
      <td>num_rentals</td>
      <td>bigint</td>
    </tr>
  </tbody>
</table>

<h3 id="business-use-cases">Business Use Cases</h3>

<p><strong>detailed_table:</strong></p>
<ul>
  <li>Useful for looking up individual rental instances by date and genre.</li>
  <li>Helps analysts or support staff answer questions like:
    <ul>
      <li>‚ÄúWhat genre was rented on a specific date?‚Äù</li>
      <li>‚ÄúHow many horror movies were rented in a specific rental instance?‚Äù</li>
    </ul>
  </li>
</ul>

<p><strong>summary_table:</strong></p>
<ul>
  <li>Supports trend and performance analysis across months.</li>
  <li>Helps stakeholders understand:
    <ul>
      <li>Which genres are most popular over time</li>
      <li>Seasonal rental trends</li>
      <li>How genre preferences shift month-to-month</li>
    </ul>
  </li>
</ul>

<p>The data is designed to be refreshed monthly, ensuring that reporting and insights remain current for ongoing business needs.</p>

<h3 id="deliverables">Deliverables</h3>

<ul>
  <li>Create reusable <strong>user-defined functions</strong> to extract year and month from timestamps.</li>
  <li>Build <strong>detailed and summary tables</strong> to structure rental data by film category and time.</li>
  <li>Implement a <strong>trigger function</strong> to auto-update summary metrics when new data is inserted.</li>
  <li>Use a <strong>stored procedure</strong> to fully refresh both tables on a monthly basis.</li>
</ul>

<h3 id="tools--skills-used">Tools &amp; Skills Used</h3>

<p><strong>Tools:</strong> PostgreSQL</p>

<p><strong>Skills:</strong></p>
<ul>
  <li>PL/pgSQL scripting</li>
  <li>Data transformation (timestamp extraction)</li>
  <li>Joins &amp; subqueries</li>
  <li>Table creation &amp; triggers</li>
  <li>Stored procedures</li>
  <li>Workflow automation using pgAgent (PostgreSQL‚Äôs job scheduler)</li>
</ul>

<h2 id="key-sql-components">Key SQL Components</h2>

<p><a href="https://github.com/nvu01/Advanced-Data-Management/blob/main/dvdrental%20project.sql" target="_blank">View full SQL script</a></p>

<h3 id="extracting-year-and-month-with-functions">Extracting Year and Month with Functions</h3>

<p>I began by creating two user-defined functions to extract month and year from <code class="language-plaintext highlighter-rouge">rental_date</code> column in <code class="language-plaintext highlighter-rouge">rental</code> table.</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">-- Create a function that extracts month from the rental_date column</span>
<span class="k">CREATE</span> <span class="k">OR</span> <span class="k">REPLACE</span> <span class="k">FUNCTION</span> <span class="n">rental_month</span> <span class="p">(</span><span class="n">rental_date</span> <span class="nb">timestamp</span> <span class="k">without</span> <span class="nb">time</span> <span class="k">zone</span><span class="p">)</span>
<span class="k">RETURNS</span> <span class="nb">int</span>
<span class="k">LANGUAGE</span> <span class="n">plpgsql</span>
<span class="k">AS</span> <span class="err">$$</span>
<span class="k">DECLARE</span> <span class="n">month_value</span> <span class="nb">int</span><span class="p">;</span>
<span class="k">BEGIN</span>
    <span class="k">SELECT</span> <span class="k">EXTRACT</span> <span class="p">(</span><span class="k">MONTH</span> <span class="k">FROM</span> <span class="n">rental_date</span><span class="p">)</span> <span class="k">INTO</span> <span class="n">month_value</span><span class="p">;</span>
    <span class="k">RETURN</span> <span class="n">month_value</span><span class="p">;</span>
<span class="k">END</span><span class="p">;</span> 
<span class="err">$$</span><span class="p">;</span>

<span class="c1">-- Create a function that extracts year from the rental_date column</span>
<span class="k">CREATE</span> <span class="k">OR</span> <span class="k">REPLACE</span> <span class="k">FUNCTION</span> <span class="n">rental_year</span> <span class="p">(</span><span class="n">rental_date</span> <span class="nb">timestamp</span> <span class="k">without</span> <span class="nb">time</span> <span class="k">zone</span><span class="p">)</span>
<span class="k">RETURNS</span> <span class="nb">int</span>
<span class="k">LANGUAGE</span> <span class="n">plpgsql</span>
<span class="k">AS</span> <span class="err">$$</span>
<span class="k">DECLARE</span> <span class="n">year_value</span> <span class="nb">int</span><span class="p">;</span>
<span class="k">BEGIN</span>
    <span class="k">SELECT</span> <span class="k">EXTRACT</span> <span class="p">(</span><span class="nb">YEAR</span> <span class="k">FROM</span> <span class="n">rental_date</span><span class="p">)</span> <span class="k">INTO</span> <span class="n">year_value</span><span class="p">;</span>
    <span class="k">RETURN</span> <span class="n">year_value</span><span class="p">;</span>
<span class="k">END</span><span class="p">;</span>
<span class="err">$$</span><span class="p">;</span>
</code></pre></div></div>

<h3 id="creating-tables">Creating Tables</h3>

<p>Next, I created <code class="language-plaintext highlighter-rouge">detailed_table</code> and <code class="language-plaintext highlighter-rouge">summary_table</code>.</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">-- Create an empty detailed table</span>
<span class="k">DROP</span> <span class="k">TABLE</span> <span class="n">IF</span> <span class="k">EXISTS</span> <span class="n">detailed_table</span><span class="p">;</span>
<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">detailed_table</span> <span class="p">(</span>
	<span class="n">rental_id</span> <span class="nb">int</span><span class="p">,</span>
	<span class="n">rental_year</span> <span class="nb">int</span><span class="p">,</span>
	<span class="n">rental_month</span> <span class="nb">int</span><span class="p">,</span>
	<span class="n">category_name</span> <span class="nb">varchar</span> <span class="p">(</span><span class="mi">25</span><span class="p">)</span>
<span class="p">);</span>

<span class="c1">-- Create a summary table that extracts and aggregates data from the detailed table</span>
<span class="k">DROP</span> <span class="k">TABLE</span> <span class="n">IF</span> <span class="k">EXISTS</span> <span class="n">summary_table</span><span class="p">;</span>
<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">summary_table</span>
<span class="k">AS</span> <span class="k">SELECT</span> <span class="n">rental_year</span><span class="p">,</span> <span class="n">rental_month</span><span class="p">,</span> <span class="n">category_name</span><span class="p">,</span> <span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="k">AS</span> <span class="n">num_rentals</span>
<span class="k">FROM</span> <span class="n">detailed_table</span>
<span class="k">GROUP</span> <span class="k">BY</span> <span class="n">rental_year</span><span class="p">,</span> <span class="n">rental_month</span><span class="p">,</span> <span class="n">category_name</span>
<span class="k">ORDER</span> <span class="k">BY</span> <span class="n">rental_year</span> <span class="k">DESC</span><span class="p">,</span> <span class="n">rental_month</span> <span class="k">DESC</span><span class="p">,</span> <span class="n">num_rentals</span> <span class="k">DESC</span><span class="p">;</span>
</code></pre></div></div>

<h3 id="trigger-for-summary-table-updates">Trigger for Summary Table Updates</h3>

<p>Once the two tables were created, I could build a trigger that updates the <code class="language-plaintext highlighter-rouge">summary_table</code> when data is inserted into the <code class="language-plaintext highlighter-rouge">detailed_table</code>.</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">-- Create a function that updates the summary table</span>
<span class="k">CREATE</span> <span class="k">OR</span> <span class="k">REPLACE</span> <span class="k">FUNCTION</span> <span class="n">summary_table_update</span><span class="p">()</span>
<span class="k">RETURNS</span> <span class="k">TRIGGER</span>
<span class="k">LANGUAGE</span> <span class="n">plpgsql</span>
<span class="k">AS</span> <span class="err">$$</span>
<span class="k">BEGIN</span>
	<span class="k">TRUNCATE</span> <span class="n">summary_table</span><span class="p">;</span>
	<span class="k">INSERT</span> <span class="k">INTO</span> <span class="n">summary_table</span>
		<span class="k">SELECT</span> <span class="n">rental_year</span><span class="p">,</span> <span class="n">rental_month</span><span class="p">,</span> <span class="n">category_name</span><span class="p">,</span> <span class="k">COUNT</span> <span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="k">AS</span> <span class="n">num_rentals</span>
		<span class="k">FROM</span> <span class="n">detailed_table</span>
		<span class="k">GROUP</span> <span class="k">BY</span> <span class="n">rental_year</span><span class="p">,</span> <span class="n">rental_month</span><span class="p">,</span> <span class="n">category_name</span>
		<span class="k">ORDER</span> <span class="k">BY</span> <span class="n">rental_year</span> <span class="k">DESC</span><span class="p">,</span> <span class="n">rental_month</span> <span class="k">DESC</span><span class="p">,</span> <span class="n">num_rentals</span> <span class="k">DESC</span><span class="p">;</span>
<span class="k">RETURN</span> <span class="k">NEW</span><span class="p">;</span>
<span class="k">END</span><span class="p">;</span> 
<span class="err">$$</span><span class="p">;</span>

<span class="c1">-- Create a trigger that updates the summary table when data is inserted into the detailed table</span>
<span class="k">DROP</span> <span class="k">TRIGGER</span> <span class="n">IF</span> <span class="k">EXISTS</span> <span class="n">update_summary_on_insert</span> <span class="k">ON</span> <span class="n">detailed_table</span><span class="p">;</span>
<span class="k">CREATE</span> <span class="k">TRIGGER</span> <span class="n">update_summary_on_insert</span>
<span class="k">AFTER</span> <span class="k">INSERT</span>
<span class="k">ON</span> <span class="n">detailed_table</span>
<span class="k">FOR</span> <span class="k">EACH</span> <span class="k">STATEMENT</span>
<span class="k">EXECUTE</span> <span class="k">PROCEDURE</span> <span class="n">summary_table_update</span><span class="p">();</span>
</code></pre></div></div>

<h3 id="populating-tables">Populating Tables</h3>

<p>Raw data was then extracted and inserted into the <code class="language-plaintext highlighter-rouge">detailed_table</code>. This would also trigger update to <code class="language-plaintext highlighter-rouge">summary_table</code>.</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">-- Add raw data to the detailed table</span>
<span class="k">INSERT</span> <span class="k">INTO</span> <span class="n">detailed_table</span>
<span class="k">SELECT</span> <span class="n">r</span><span class="p">.</span><span class="n">rental_id</span><span class="p">,</span> <span class="n">rental_year</span> <span class="p">(</span><span class="n">r</span><span class="p">.</span><span class="n">rental_date</span><span class="p">),</span> <span class="n">rental_month</span> <span class="p">(</span><span class="n">r</span><span class="p">.</span><span class="n">rental_date</span><span class="p">),</span> <span class="k">c</span><span class="p">.</span><span class="n">name</span> <span class="k">AS</span> <span class="n">category_name</span>
<span class="k">FROM</span> <span class="n">rental</span> <span class="n">r</span>
<span class="k">INNER</span> <span class="k">JOIN</span> <span class="n">inventory</span> <span class="n">i</span> <span class="k">ON</span> <span class="n">r</span><span class="p">.</span><span class="n">inventory_id</span> <span class="o">=</span> <span class="n">i</span><span class="p">.</span><span class="n">inventory_id</span>
<span class="k">INNER</span> <span class="k">JOIN</span> <span class="n">film_category</span> <span class="n">fc</span> <span class="k">ON</span> <span class="n">i</span><span class="p">.</span><span class="n">film_id</span> <span class="o">=</span> <span class="n">fc</span><span class="p">.</span><span class="n">film_id</span>
<span class="k">INNER</span> <span class="k">JOIN</span> <span class="n">category</span> <span class="k">c</span> <span class="k">ON</span> <span class="n">fc</span><span class="p">.</span><span class="n">category_id</span> <span class="o">=</span> <span class="k">c</span><span class="p">.</span><span class="n">category_id</span>
<span class="k">ORDER</span> <span class="k">BY</span> <span class="n">rental_year</span> <span class="k">DESC</span><span class="p">,</span> <span class="n">rental_month</span> <span class="k">DESC</span><span class="p">;</span>
</code></pre></div></div>

<h3 id="stored-procedure-to-refresh-all-data">Stored Procedure to Refresh All Data</h3>

<p>Finally, I created a procedure that refreshes data in both detailed and summary tables.</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">-- Create a procedure that re-populates the 2 tables with raw data from the database</span>
<span class="c1">-- Any test data that was manually inserted to the detailed table won't be included</span>
<span class="k">DROP</span> <span class="k">PROCEDURE</span> <span class="n">IF</span> <span class="k">EXISTS</span> <span class="n">refresh_tables</span><span class="p">();</span>
<span class="k">CREATE</span> <span class="k">PROCEDURE</span> <span class="n">refresh_tables</span> <span class="p">()</span>
<span class="k">LANGUAGE</span> <span class="n">plpgsql</span>
<span class="k">AS</span> <span class="err">$$</span>
<span class="k">BEGIN</span>
	<span class="k">TRUNCATE</span> <span class="n">detailed_table</span><span class="p">;</span>
	<span class="k">TRUNCATE</span> <span class="n">summary_table</span><span class="p">;</span>
	<span class="c1">-- Temporarily disable trigger</span>
    <span class="k">ALTER</span> <span class="k">TABLE</span> <span class="n">detailed_table</span> <span class="n">DISABLE</span> <span class="k">TRIGGER</span> <span class="n">update_summary_on_insert</span><span class="p">;</span>
	<span class="c1">-- Update detailed_table</span>
	<span class="k">INSERT</span> <span class="k">INTO</span> <span class="n">detailed_table</span>
		<span class="k">SELECT</span> <span class="n">r</span><span class="p">.</span><span class="n">rental_id</span><span class="p">,</span> <span class="n">rental_year</span> <span class="p">(</span><span class="n">ym</span><span class="p">.</span><span class="n">rental_date</span><span class="p">),</span> <span class="n">rental_month</span> <span class="p">(</span><span class="n">ym</span><span class="p">.</span><span class="n">rental_date</span><span class="p">),</span> <span class="k">c</span><span class="p">.</span><span class="n">name</span> <span class="k">AS</span> <span class="n">category_name</span>
		<span class="k">FROM</span> <span class="n">rental</span> <span class="n">r</span>
		<span class="k">INNER</span> <span class="k">JOIN</span> <span class="n">inventory</span> <span class="n">i</span> <span class="k">ON</span> <span class="n">r</span><span class="p">.</span><span class="n">inventory_id</span> <span class="o">=</span> <span class="n">i</span><span class="p">.</span><span class="n">inventory_id</span>
		<span class="k">INNER</span> <span class="k">JOIN</span> <span class="n">film_category</span> <span class="n">fc</span> <span class="k">ON</span> <span class="n">i</span><span class="p">.</span><span class="n">film_id</span> <span class="o">=</span> <span class="n">fc</span><span class="p">.</span><span class="n">film_id</span>
		<span class="k">INNER</span> <span class="k">JOIN</span> <span class="n">category</span> <span class="k">c</span> <span class="k">ON</span> <span class="n">fc</span><span class="p">.</span><span class="n">category_id</span> <span class="o">=</span> <span class="k">c</span><span class="p">.</span><span class="n">category_id</span>
		<span class="k">INNER</span> <span class="k">JOIN</span> <span class="n">rental</span> <span class="n">ym</span> <span class="k">ON</span> <span class="n">r</span><span class="p">.</span><span class="n">rental_id</span> <span class="o">=</span> <span class="n">ym</span><span class="p">.</span><span class="n">rental_id</span>
		<span class="k">ORDER</span> <span class="k">BY</span> <span class="n">rental_year</span> <span class="k">DESC</span><span class="p">,</span> <span class="n">rental_month</span> <span class="k">DESC</span><span class="p">;</span>
	<span class="c1">-- Update summary_table</span>
	<span class="k">INSERT</span> <span class="k">INTO</span> <span class="n">summary_table</span>
		<span class="k">SELECT</span> <span class="n">rental_year</span><span class="p">,</span> <span class="n">rental_month</span><span class="p">,</span> <span class="n">category_name</span><span class="p">,</span> <span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="k">AS</span> <span class="n">num_rentals</span>
		<span class="k">FROM</span> <span class="n">detailed_table</span>
		<span class="k">GROUP</span> <span class="k">BY</span> <span class="n">rental_year</span><span class="p">,</span> <span class="n">rental_month</span><span class="p">,</span> <span class="n">category_name</span>
		<span class="k">ORDER</span> <span class="k">BY</span> <span class="n">rental_year</span> <span class="k">DESC</span><span class="p">,</span> <span class="n">rental_month</span> <span class="k">DESC</span><span class="p">,</span> <span class="n">num_rentals</span> <span class="k">DESC</span><span class="p">;</span>
	<span class="c1">-- Re-enable trigger</span>
    <span class="k">ALTER</span> <span class="k">TABLE</span> <span class="n">detailed_table</span> <span class="n">ENABLE</span> <span class="k">TRIGGER</span> <span class="n">update_summary_on_insert</span><span class="p">;</span>
<span class="k">END</span><span class="p">;</span>
<span class="err">$$</span><span class="p">;</span>
</code></pre></div></div>

<h2 id="automation-with-pgagent">Automation with pgAgent</h2>

<p>To keep the data fresh for reporting, the stored procedure can be scheduled to run monthly using pgAgent, PostgreSQL‚Äôs built-in job scheduler. This ensures the summary reflects the most recent rental data without manual intervention.</p>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>This project highlights the power of SQL and PostgreSQL for preparing data in a way that supports scalable and automated business intelligence workflows. By creating a clean, transformed layer of data ready for analysis, I can bridge the gap between raw transactions and valuable insights. These structured tables can now be easily integrated with tools like Power BI or Tableau for dashboarding, trend analysis, and storytelling with data.</p>]]></content><author><name>Ngan Vu</name><email>&lt;nganvu2601@gmail.com&gt;</email></author><category term="featured_projects" /><category term="Waggle" /><category term="Power BI report" /><category term="data visualization" /><category term="storytelling" /><category term="LapDog" /><category term="LapCat" /><summary type="html"><![CDATA[This project uses PostgreSQL and PL/pgSQL to extract, transform and organize DVD rental data to support future business reporting. It demonstrates skills in SQL functions, triggers, and stored procedures.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://nvu01.github.io//assets/img/posts/2/PostgreSQL.png" /><media:content medium="image" url="https://nvu01.github.io//assets/img/posts/2/PostgreSQL.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">U.S. YouTube Video Trends: A Tableau Dashboard &amp;amp; Story</title><link href="https://nvu01.github.io//featured_projects/2025-05-28-youtube-video-trends/" rel="alternate" type="text/html" title="U.S. YouTube Video Trends: A Tableau Dashboard &amp;amp; Story" /><published>2025-05-28T00:00:00+00:00</published><updated>2025-11-20T23:52:05+00:00</updated><id>https://nvu01.github.io//featured_projects/youtube-video-trends</id><content type="html" xml:base="https://nvu01.github.io//featured_projects/2025-05-28-youtube-video-trends/"><![CDATA[<p>What do trending YouTube videos reveal about American viewers? In this Tableau project, I analyzed U.S. YouTube trending data to uncover patterns in viewer sentiment, identify top-performing creators, and explore regional content preferences.</p>

<p><a href="https://github.com/nvu01/Tableau-Public-Dashboard" target="_blank" rel="noopener">
  <i class="icon-github"></i> GitHub Repo
</a></p>

<ul class="large-only" id="markdown-toc">
  <li><a href="#data-source" id="markdown-toc-data-source">Data Source</a></li>
  <li><a href="#what-i-set-out-to-discover" id="markdown-toc-what-i-set-out-to-discover">What I Set Out to Discover</a></li>
  <li><a href="#interactive-dashboard" id="markdown-toc-interactive-dashboard">Interactive Dashboard</a></li>
  <li><a href="#guided-story-a-walkthrough-of-insights" id="markdown-toc-guided-story-a-walkthrough-of-insights">Guided Story: A Walkthrough of Insights</a></li>
  <li><a href="#key-takeaways" id="markdown-toc-key-takeaways">Key Takeaways</a></li>
  <li><a href="#behind-the-visualizations" id="markdown-toc-behind-the-visualizations">Behind the Visualizations</a></li>
  <li><a href="#final-thoughts" id="markdown-toc-final-thoughts">Final Thoughts</a></li>
</ul>

<h2 id="data-source">Data Source</h2>

<p>For my data visualization project, I am using datasets containing statistics on trending YouTube videos in the U.S. These datasets were originally sourced from Kaggle and then transformed and cleaned by Udacity for educational purposes. The cleaned datasets can be found in my <a href="https://github.com/nvu01/Tableau-Public-Dashboard/tree/main">GitHub repo</a>.</p>

<p>Data source: <a href="https://www.kaggle.com/datasets/datasnaek/youtube-new/data?select=USvideos.csv">Kaggle</a></p>

<h2 id="what-i-set-out-to-discover">What I Set Out to Discover</h2>

<p>I designed four interactive visualizations to tell a story and answer deeper questions that go beyond surface metrics like views or likes. These business questions guided the structure and design of the dashboard:</p>
<ul>
  <li>Do more popular videos also attract more criticism?</li>
  <li>Which creators consistently break through the noise?</li>
  <li>Are some categories winning because of volume, or higher per-video impact?</li>
  <li>How does content preference vary across the U.S.?</li>
</ul>

<h2 id="interactive-dashboard">Interactive Dashboard</h2>

<p>Let‚Äôs explore the full dashboard here! Filter by category, hover over the charts for tooltips, and uncover deeper insights as you interact with the data.</p>

<p>üëâ <a href="https://public.tableau.com/views/U_S_YouTubeVideoTrends/U_S_YouTubeVideoTrends?:showVizHome=no&amp;:embed=true" target="_blank" rel="noopener">Open the dashboard in new tab</a>.</p>

<iframe src="https://public.tableau.com/views/U_S_YouTubeVideoTrends/U_S_YouTubeVideoTrends?:showVizHome=no&amp;:embed=true" width="1280" height="725" frameborder="0" allowfullscreen="">
</iframe>

<h2 id="guided-story-a-walkthrough-of-insights">Guided Story: A Walkthrough of Insights</h2>

<p>This Tableau Story walks you through the key findings, one insight at a time.</p>

<p>üëâ <a href="https://public.tableau.com/views/StoryU_S_YouTubeVideoTrends/Story?:showVizHome=no&amp;:embed=true" target="_blank" rel="noopener">Open the dashboard in new tab</a>.</p>

<iframe src="https://public.tableau.com/views/StoryU_S_YouTubeVideoTrends/Story?:showVizHome=no&amp;:embed=true" width="1000" height="827px" frameborder="0" allowfullscreen="">
</iframe>

<h2 id="key-takeaways">Key Takeaways</h2>

<ul>
  <li><strong>Popularity attracts polarity.</strong> Trending videos with high likes often come with a high number of dislikes.</li>
  <li><strong>Big brands dominate.</strong> Marvel Entertainment and YouTube Spotlight top the charts in total views.</li>
  <li><strong>Music wins in impact.</strong> Fewer videos, but each with massive reach.</li>
  <li><strong>Geography matters.</strong> Music reigns nationwide, but Entertainment and Gaming see regional spikes.</li>
</ul>

<h2 id="behind-the-visualizations">Behind the Visualizations</h2>

<p>The analysis is powered by four main visualizations:</p>

<p><strong>1. Top 20 YouTube Channels (Bar Chart)</strong>:</p>

<p>To identify the most influential creators, I built a bar chart ranking the top 20 YouTube channels by total views. Since trending videos in the dataset appear repeatedly across multiple dates (as long as it remained trending), I used Tableau‚Äôs Level of Detail (LOD) calculation to extract the maximum value of views per unique video, avoiding duplicate entries. Then I aggregated these values to get the total views per channel.</p>

<p><strong>2. Likes vs. Dislikes Across YouTube Categories (Scatter Plot)</strong>:</p>

<p>The scatter plot reveals the relationship between likes and dislikes for trending videos in different categories. Each data point represents an individual video. The shape of each point corresponds to the video‚Äôs category, so users can immediately identify content types without relying on color alone.</p>

<p>Because the same video could appear on multiple trending days, I again applied an LOD calculation to capture only each video‚Äôs highest recorded engagement (likes and dislikes), instead of a cumulative total across multiple entries.</p>

<p>I also added a ‚ÄúCategory Name‚Äù filter to let users focus on specific genres (like Comedy or Music) and investigate how sentiment patterns vary.</p>

<p><strong>3. Video and View Counts by Category (Dual-Axis Bar Chart)</strong>:</p>

<p>This chart addresses an important question: is volume or impact more important in content trends?</p>

<p>I created a dual-axis chart to show:</p>
<ul>
  <li>Bar chart: Total view counts per category (aggregated from highest view count per video)</li>
  <li>Dot plot: Number of unique trending videos in that category</li>
</ul>

<p>I also structured the hierarchy with ‚ÄúChannel Title‚Äù nested under ‚ÄúCategory Name‚Äù, so users can drill down into specific creators within each genre.</p>

<p><strong>4. Top YouTube Categories by State (Interactive Map)</strong>:</p>

<p>The interactive map shows regional preferences with each state colored by its most popular YouTube category (based on total views). This visualization offers a regional perspective on video trends and helps understand geographic differences in content preferences.</p>

<p>Users can also apply the ‚ÄúCategory Name‚Äù filter to see where specific content types are trending, even if they‚Äôre not the most dominant in that state. This interaction makes it easy to investigate niche audiences or regional differences in taste.</p>

<p><strong>Tooltips</strong></p>

<p>Every visualization includes tooltips that appear when users hover over data points. These tooltips provide additional context such as video titles, channel names, categories, view counts, likes, and dislikes, without cluttering the visual space. This design choice keeps the interface clean while giving users quick access to details for deeper insight.</p>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>With Tableau, I turned a complex dataset into an accessible, interactive tool that surfaces insights through design and exploration. But this project was more than just building a dashboard. It was an exercise in turning raw data into narrative. Beyond the technical skills, this project pushed me to think like a storyteller, to guide users through the data, not just analyze it.</p>]]></content><author><name>Ngan Vu</name><email>&lt;nganvu2601@gmail.com&gt;</email></author><category term="featured_projects" /><category term="youtube" /><category term="tableau dashboard" /><category term="tableau story" /><category term="data visualization" /><category term="storytelling" /><summary type="html"><![CDATA[An interactive Tableau dashboard that visualizes U.S. YouTube trending data to reveal viewer sentiment, top channels, and regional content preferences.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://nvu01.github.io//assets/img/posts/1/eyestetix-studio-Bm-5o5M2mAI-unsplash.jpg" /><media:content medium="image" url="https://nvu01.github.io//assets/img/posts/1/eyestetix-studio-Bm-5o5M2mAI-unsplash.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>